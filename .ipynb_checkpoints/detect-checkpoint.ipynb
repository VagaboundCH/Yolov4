{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from util.ipynb\n",
      "torch.Size([2, 4])\n",
      "torch.Size([1, 2, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 4, 1])\n",
      "importing Jupyter notebook from model.ipynb\n",
      "importing Jupyter notebook from img_process.ipynb\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import import_ipynb\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "from model import Network\n",
    "from img_process import preprocess_img, inp_to_image\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3\n",
      "Network loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1749: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  0\n",
      "batch_size:  1\n",
      "Objects Detected:    person bed\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  1\n",
      "batch_size:  1\n",
      "Objects Detected:    bicycle truck dog\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  2\n",
      "batch_size:  1\n",
      "Objects Detected:    bird\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  3\n",
      "batch_size:  1\n",
      "Objects Detected:    zebra giraffe\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  4\n",
      "batch_size:  1\n",
      "Objects Detected:    person\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  5\n",
      "batch_size:  1\n",
      "Objects Detected:    horse horse horse horse\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  6\n",
      "batch_size:  1\n",
      "Objects Detected:    person person person person person person person person person person person umbrella\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  7\n",
      "batch_size:  1\n",
      "Objects Detected:    person\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  8\n",
      "batch_size:  1\n",
      "Objects Detected:    train\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  9\n",
      "batch_size:  1\n",
      "Objects Detected:    car car car car car car car truck traffic light\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  10\n",
      "batch_size:  1\n",
      "Objects Detected:    chair chair chair clock\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  11\n",
      "batch_size:  1\n",
      "Objects Detected:    person person\n",
      "----------------------------------------------------------\n",
      "prediction:  torch.Size([1, 22743, 85])\n",
      "torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 85])\n",
      "max_conf:  torch.Size([22743, 1])\n",
      "max_conf_score:  torch.Size([22743, 1])\n",
      "image_pred:  torch.Size([22743, 7])\n",
      "im_id:  0\n",
      "i:  12\n",
      "batch_size:  1\n",
      "Objects Detected:    person dog horse\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def arg_parse():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='YOLOv4 ')\n",
    "   \n",
    "    parser.add_argument(\"--images\", dest = 'images', help = \n",
    "                        \"Image / Directory containing input images\",\n",
    "                        default = \"images\", type = str)\n",
    "    parser.add_argument(\"--result\", dest = 'result', help = \n",
    "                        \" Directory to store results \",\n",
    "                        default = \"result\", type = str)\n",
    "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
    "\n",
    "    parser.add_argument(\"--bs\", dest = \"bs\", help = \"Batch size\", default = 1)\n",
    "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Detection Confidence \", default = 0.5)\n",
    "    parser.add_argument(\"--cfg\", dest = 'configfile', help = \n",
    "                        \"Config file\",\n",
    "                        default = \"cfg/yolov4.cfg\", type = str)\n",
    "    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n",
    "                        \"weightsfile\",\n",
    "                        default = \"C:/Users/User/weights/yolov4.weights\", type = str)\n",
    "    parser.add_argument(\"--reso\", dest = 'resolution', help = \n",
    "                        \"Input resolution of the network\",\n",
    "                        default = \"608\", type = str)\n",
    "    parser.add_argument(\"--scales\", dest = \"scales\", help = \"Scales to use for detection\",\n",
    "                        default = \"1,2,3\", type = str)   \n",
    "    parser.add_argument(\"-f\") # Quick fix for Jupyter Notebook\n",
    "    return parser.parse_args()\n",
    "    \n",
    "#     args = (\"--images\", dest = 'images', help = \"Image / Directory containing input images\",default = \"images\", type = str,\n",
    "#             \"--result\", dest = 'result', help = \" Directory to store results \", default = \"result\", type = str,\n",
    "#             \"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4, \n",
    "#             \"--bs\", dest = \"bs\", help = \"Batch size\", default = 1, \"--confidence\", dest = \"confidence\", help = \"Detection Confidence \", default = 0.5,\n",
    "#             \"--cfg\", dest = 'configfile', help = \"Config file\", default = \"cfg/yolov3.cfg\", type = str,\n",
    "#             \"--weights\", dest = 'weightsfile', help = \"weightsfile\", default = \"C:/Users/User/weights/yolov4.weights\", type = str,\n",
    "#             \"--reso\", dest = 'resolution', help = \"Input resolution of the network\", default = \"256\", type = str,\n",
    "#             \"--scales\", dest = \"scales\", help = \"Scales to use for detection\", default = \"1,2,3\", type = str)\n",
    "    \n",
    "#     return arg_parse(*args)\n",
    "    \n",
    "\n",
    "if __name__ ==  '__main__':\n",
    "    args = arg_parse()\n",
    "    \n",
    "    scales = args.scales\n",
    "    print(scales)\n",
    " \n",
    "    images = args.images\n",
    "    batch_size = int(args.bs)\n",
    "    confidence = float(args.confidence)\n",
    "    nms_thesh = float(args.nms_thresh)\n",
    "    start = 0\n",
    "\n",
    "    CUDA = torch.cuda.is_available()\n",
    "\n",
    "    num_classes = 80\n",
    "    classes = load_classes('data/coco.names') \n",
    "\n",
    "    model = Network(args.configfile)\n",
    "    model.load_weights(args.weightsfile)\n",
    "    print(\"Network loaded\")\n",
    "    \n",
    "    model.dn_info[\"height\"] = args.resolution\n",
    "    in_dim = int(model.dn_info[\"height\"])\n",
    "\n",
    "\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    read_dir = time.time()\n",
    "    try:\n",
    "        imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images) if os.path.splitext(img)[1] == '.png' or os.path.splitext(img)[1] =='.jpeg' or os.path.splitext(img)[1] =='.jpg']\n",
    "    except NotADirectoryError:\n",
    "        imlist = []\n",
    "        imlist.append(osp.join(osp.realpath('.'), images))\n",
    "    except FileNotFoundError:\n",
    "        print (\"No with the name {}\".format(images))\n",
    "        exit()\n",
    "        \n",
    "    if not os.path.exists(args.result):\n",
    "        os.makedirs(args.result)\n",
    "            \n",
    "    batches = list(map(preprocess_img, imlist, [in_dim for x in range(len(imlist))]))\n",
    "    im_batches = [x[0] for x in batches]\n",
    "    orig_ims = [x[1] for x in batches]\n",
    "    im_dim_list = [x[2] for x in batches]\n",
    "    #Explain\n",
    "    im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if CUDA:\n",
    "        im_dim_list = im_dim_list.cuda()\n",
    "    \n",
    "    leftover = 0\n",
    "    \n",
    "    if (len(im_dim_list) % batch_size):\n",
    "        leftover = 1\n",
    "        \n",
    "\n",
    "    i = 0\n",
    "    \n",
    "\n",
    "    write = False\n",
    "    \n",
    "    \n",
    "    objs = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    for batch in im_batches:\n",
    "        if CUDA:\n",
    "            batch = batch.cuda()\n",
    "        #print('batch size => ', batch.size())\n",
    "        with torch.no_grad():\n",
    "            prediction = model(batch, CUDA)\n",
    "        \n",
    "        print('prediction: ',prediction.size())\n",
    "        \n",
    "        \n",
    "        prediction = write_results(prediction, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
    "\n",
    "        \n",
    "        if type(prediction) == int:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "            \n",
    "        #Add the current batch number\n",
    "        prediction[:,0] += i*batch_size\n",
    "        \n",
    "    \n",
    "            \n",
    "          \n",
    "        if not write:\n",
    "            output = prediction\n",
    "            write = 1\n",
    "        else:\n",
    "            output = torch.cat((output,prediction))\n",
    "\n",
    "\n",
    "\n",
    "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "            print('im_id: ', im_num)\n",
    "            print('i: ', i)\n",
    "            print('batch_size: ', batch_size)\n",
    "            im_id = i*batch_size + im_num\n",
    "            objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
    "            print(\"----------------------------------------------------------\")\n",
    "#             confidences = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "        if CUDA:\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    try:\n",
    "        output\n",
    "    except NameError:\n",
    "        print(\"No detections were made\")\n",
    "        exit()\n",
    "        \n",
    "    im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
    "    \n",
    "    scaling_factor = torch.min(in_dim/im_dim_list,1)[0].view(-1,1)\n",
    "    \n",
    "    \n",
    "    output[:,[1,3]] -= (in_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
    "    output[:,[2,4]] -= (in_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
    "    \n",
    "    \n",
    "    \n",
    "    output[:,1:5] /= scaling_factor\n",
    "    \n",
    "    for i in range(output.shape[0]):\n",
    "        output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
    "        output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
    "        \n",
    "#     colors = pkl.load(open(\"pallete\", \"rb\"))\n",
    "    with open(\"pallete\", \"rb\") as pkl_file:\n",
    "        colors = pkl.load(pkl_file)\n",
    "    \n",
    "    def write(x, batches, results):\n",
    "        c1 = tuple(x[1:3].int())\n",
    "        c2 = tuple(x[3:5].int())\n",
    "        img = results[int(x[0])]\n",
    "        cls = int(x[-1])\n",
    "        label = \"{0}\".format(classes[cls])\n",
    "        color = random.choice(colors)\n",
    "        cv2.rectangle(img, c1, c2,color, 1)\n",
    "        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "        c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4 \n",
    "        cv2.rectangle(img, c1, c2,color, -1)\n",
    "        cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1)\n",
    "        return img\n",
    "    \n",
    "            \n",
    "    list(map(lambda x: write(x, im_batches, orig_ims), output))\n",
    "      \n",
    "    det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(args.result,x.split(\"\\\\\")[-1]))\n",
    "    \n",
    "    list(map(cv2.imwrite, det_names, orig_ims))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0., nan., nan.,  ..., nan., nan.,  79.],\n",
      "        [  0., nan., nan.,  ..., nan., nan.,   0.],\n",
      "        [  0., nan., nan.,  ..., nan., nan.,   0.],\n",
      "        ...,\n",
      "        [  0., nan., nan.,  ..., nan., nan.,   0.],\n",
      "        [  0., nan., nan.,  ..., nan., nan.,   0.],\n",
      "        [  0., nan., nan.,  ..., nan., nan.,   0.]], device='cuda:0') 0.5 80\n"
     ]
    }
   ],
   "source": [
    "print(prediction, confidence, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a995d3d976aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mim_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-a995d3d976aa>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobjs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mim_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  torch.Size([384, 8])\n",
      "prediction:  torch.Size([192, 8])\n",
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "80\n",
      "tensor([[  1., nan., nan.,  ..., nan., nan.,  79.],\n",
      "        [  1., nan., nan.,  ..., nan., nan.,   0.],\n",
      "        [  1., nan., nan.,  ..., nan., nan.,   0.],\n",
      "        ...,\n",
      "        [  1., nan., nan.,  ..., nan., nan.,   0.],\n",
      "        [  1., nan., nan.,  ..., nan., nan.,   0.],\n",
      "        [  1., nan., nan.,  ..., nan., nan.,   0.]], device='cuda:0')\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print('output: ',output.size())\n",
    "print('prediction: ',prediction.size())\n",
    "\n",
    "print(classes)\n",
    "print(len(classes))\n",
    "print(prediction)\n",
    "print(confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
